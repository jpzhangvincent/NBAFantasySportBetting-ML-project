{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "import pickle\n",
    "import pandas as pd"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "from datetime import datetime"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "with open('../Data/allplayerFantasyGameLogs.pickle', 'rb') as handle:\n",
    "  allplayerFantasyGameLogs = pickle.load(handle)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "allplayerFantasyGameLogs.set_index('GAME_DATE', inplace=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##Data Integration"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We will split the allplayerGameLogs into training and test sets. And we need to use two helpful functions to extract player-level and team-level features to train the prediction model for fantasy points from a player. We also need another helpful function to aggregate corresponding information from the test set for the testing purpose."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 60,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def aggr(group):\n",
    "        group['NumGames'] = group['fullName'].count()\n",
    "        group['AvgFanPTs'] = group['FanPTs'].mean()\n",
    "        group['AvgPTS'] = group['PTS'].mean()\n",
    "        group['AvgMIN'] = group['MIN'].mean() \n",
    "        group['AvgFGM'] = group['FGM'].mean()\n",
    "        group['AvgFGA'] = group['FGA'].mean()\n",
    "        group['AvgFG3M'] = group['FG3M'].mean()\n",
    "        group['AvgFG3A'] = group['FG3A'].mean()\n",
    "        group['AvgREB'] = group['REB'].mean()\n",
    "        group['AvgAST'] = group['AST'].mean()\n",
    "        group['AvgSTL'] = group['STL'].mean()\n",
    "        group['AvgTOV'] = group['TOV'].mean()\n",
    "        group['AvgPLUS_MINUS'] = group['PLUS_MINUS'].mean()\n",
    "        group['NumDouBL'] = group['DouBL'].sum()\n",
    "        group['NumTriBL'] = group['TriBL'].sum()\n",
    "\n",
    "        group['Last3GameAvgFanPTs'] = group['FanPTs'][:3].mean()\n",
    "        group['Last6GameAvgFanPTs'] = group['FanPTs'][:6].mean()\n",
    "        \n",
    "        group['Last3GameAvgMIN'] = group['MIN'][:3].mean()\n",
    "        group['Last3GameAvgPTS'] = group['PTS'][:3].mean()\n",
    "        return group\n",
    "    \n",
    "def aggr_stats(date,allplayerFantasyGameLogs):\n",
    "    interest_columns = ['fullName','Player_ID','Team','position1','MIN','PTS','FGM','FGA', 'FG3M','FG3A', \\\n",
    "                        'REB','AST','STL','TOV','PLUS_MINUS','DouBL','TriBL','FanPTs']\n",
    "    tmp = allplayerFantasyGameLogs.ix['2015-10-27':date]\n",
    "    \n",
    "    playerID_tmp = tmp.reset_index().copy()\n",
    "    tmp.grouped = playerID_tmp[interest_columns].groupby('Player_ID')\n",
    "    \n",
    "    Newdf = tmp.grouped.apply(aggr)\n",
    "    Newdf = Newdf.drop(['MIN','PTS','FGM','FGA', 'FG3M','FG3A','REB','AST','STL','TOV','PLUS_MINUS','DouBL','TriBL','FanPTs'],axis=1)\n",
    "    Newdf.drop_duplicates(inplace=True)\n",
    "    \n",
    "    bins = [0, 10, 20, 30, 40, 100]\n",
    "    group_names = ['benchPlayer','belowAvg','average','advanced','top']\n",
    "    Newdf['Rank']= pd.cut(Newdf['AvgFanPTs'],bins,labels=group_names)\n",
    "    \n",
    "    return(Newdf)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 54,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def aggr_teamVSteam(group):\n",
    "        group['TeamStdVSFanPTs'] = group['FanPTs'].std()\n",
    "        group['TeamAvgVSFanPTs'] = group['FanPTs'].mean()\n",
    "        group['TeamMaxVSFanPTs'] = group['FanPTs'].max()\n",
    "        return group\n",
    "\n",
    "def aggr_team(group):\n",
    "        group['TeamStdFanPTs'] = group['TeamStdVSFanPTs'].mean()\n",
    "        group['TeamAvgFanPTs'] = group['TeamAvgVSFanPTs'].mean()\n",
    "        group['TeamMaxFanPTs'] = group['TeamMaxVSFanPTs'].mean()\n",
    "        return group    \n",
    "\n",
    "def generate_team_features(playerGameLogs, playerFeatureTable, date):\n",
    "    tmp = playerGameLogs['2015-10-27': date]\n",
    "    tmp = tmp.reset_index()\n",
    "    bad_players = playerFeatureTable[playerFeatureTable.Rank=='benchPlayer']['Player_ID']\n",
    "    interest_cols = ['fullName','Player_ID','Team','OpponentTeam','position1','FanPTs','MIN']\n",
    "    tmp = tmp[interest_cols]\n",
    "    tmp = tmp[~tmp['Player_ID'].isin(bad_players)]\n",
    "    \n",
    "    newdf = tmp.copy()\n",
    "    newdf_grouped = newdf.groupby(['Team','OpponentTeam'])\n",
    "        \n",
    "    Newdf = newdf_grouped.apply(aggr_teamVSteam)\n",
    "    Newdf.drop(['fullName','Player_ID','MIN','FanPTs','position1'],inplace=True,axis=1)\n",
    "    Newdf.drop_duplicates(['Team','OpponentTeam'],inplace=True)\n",
    "    \n",
    "    Newdf.drop('OpponentTeam',axis=1,inplace=True)\n",
    "    \n",
    "    Newdf2 = Newdf.copy()\n",
    "    Newdf2_grouped = Newdf2.groupby('Team')\n",
    "    \n",
    "    Newdf_overall = Newdf2_grouped.apply(aggr_team)\n",
    "    Newdf_overall.drop(['TeamStdVSFanPTs','TeamAvgVSFanPTs','TeamMaxVSFanPTs'],inplace=True,axis=1)\n",
    "    Newdf_overall.drop_duplicates('Team',inplace=True)\n",
    "    \n",
    "    return(Newdf_overall)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 64,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def drop_y(df):\n",
    "    # list comprehension of the cols that end with '_y'\n",
    "    to_drop = [x for x in df if x.endswith('_y')]\n",
    "    df.drop(to_drop, axis=1, inplace=True)\n",
    "\n",
    "def rename_x(df):\n",
    "    for col in df:\n",
    "        if col.endswith('_x'):\n",
    "            df.rename(columns={col:col.rstrip('_x')}, inplace=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 82,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def get_train_test(train_date, test_date): #format like'2/10/2016'\n",
    "    train_date_index = pd.date_range(start='11/1/2015', end=train_date, freq='D')\n",
    "    train_df = pd.DataFrame()\n",
    "    \n",
    "    alldates = allplayerFantasyGameLogs.index\n",
    "    trydates = pd.date_range(start='10/27/2015', end='2/29/2016', freq='D')\n",
    "    s = set(alldates)\n",
    "    nodates = [x for x in trydates if x not in s]\n",
    "    \n",
    "    for idx in train_date_index:\n",
    "        tmp_idx = idx+1\n",
    "        if tmp_idx not in nodates and idx not in nodates:\n",
    "            #aggregate the statistics from players -> player-level features\n",
    "            trainLogs = allplayerFantasyGameLogs.ix['2015-10-27':idx]\n",
    "            train_player_df = aggr_stats(idx,trainLogs)   \n",
    "            #next we need to collect the player's next game Fantasy Points.\n",
    "            next_date = idx + 1\n",
    "            tmpLogs = allplayerFantasyGameLogs[['fullName', 'Player_ID','Team','OpponentTeam','FanPTs']].ix[next_date]\n",
    "            tmpLogs.rename(columns={'FanPTs':'NewGameFanPTs'},inplace=True)\n",
    "            #join the tmpLogs and player festure table by Player_ID, which is based on the players on a new game day\n",
    "            newgame_df = pd.merge(tmpLogs,train_player_df,how='inner',on='Player_ID')\n",
    "            drop_y(newgame_df)\n",
    "            rename_x(newgame_df)\n",
    "\n",
    "            #get the team features table \n",
    "            train_team_df = generate_team_features(allplayerFantasyGameLogs, train_player_df, idx)\n",
    "            newgame_df = pd.merge(newgame_df,train_team_df,how='left',on='Team')\n",
    "            train_df = pd.concat([train_df,newgame_df],axis=0)\n",
    "\n",
    "    test_date_index = pd.date_range(start=train_date, end=test_date, freq='D')[1:]\n",
    "    start_test_date = pd.date_range(start=train_date, end=test_date, freq='D')[0]\n",
    "    test_df = pd.DataFrame()\n",
    "    for idx in test_date_index:\n",
    "        tmp_idx = idx+1\n",
    "        if tmp_idx not in nodates and idx not in nodates:\n",
    "            #aggregate the statistics from players -> player-level features\n",
    "            testLogs = allplayerFantasyGameLogs.ix[start_test_date:idx]\n",
    "            test_player_df = aggr_stats(idx,testLogs)   \n",
    "            #next we need to collect the player's next game Fantasy Points.\n",
    "            next_date = idx + 1\n",
    "            tmpLogs = allplayerFantasyGameLogs[['fullName', 'Player_ID','Team','OpponentTeam','FanPTs']].ix[next_date]\n",
    "            tmpLogs.rename(columns={'FanPTs':'NewGameFanPTs'},inplace=True)\n",
    "            #join the tmpLogs and player festure table by Player_ID, which is based on the players on a new game day\n",
    "            newgame_df = pd.merge(tmpLogs,train_player_df,how='inner',on='Player_ID')\n",
    "            drop_y(newgame_df)\n",
    "            rename_x(newgame_df)\n",
    "\n",
    "            #get the team features table \n",
    "            test_team_df = generate_team_features(allplayerFantasyGameLogs, test_player_df, idx)\n",
    "            newgame_df = pd.merge(newgame_df,test_team_df,how='left',on='Team')\n",
    "            test_df = pd.concat([test_df,newgame_df],axis=0) \n",
    "        \n",
    "    return(train_df, test_df)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 83,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "train_set, test_set = get_train_test('2/20/2016', '2/28/2016')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 86,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(16384, 29)"
      ]
     },
     "execution_count": 86,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "train_set.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 87,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(1269, 29)"
      ]
     },
     "execution_count": 87,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "test_set.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 88,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "with open('../Data/train_set_02_29.pickle', 'wb') as handle:\n",
    "  pickle.dump(train_set, handle)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 89,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "with open('../Data/test_set_02_29.pickle', 'wb') as handle:\n",
    "  pickle.dump(test_set, handle)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##Prepocessing"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##Modeling"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##Testing"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 2",
   "language": "python",
   "name": "python2"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.11"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}
